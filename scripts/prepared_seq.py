# -*- coding: utf-8 -*-
"""prepared_sequences.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hao9rf0EsOWaGpiuyXs7VzmNS8CcUG-h
"""

import zipfile
import os

zip_filename = '/content/cap_prep.zip'
extract_path = '/content/unzipped'

# Create extraction folder if it doesn't exist
os.makedirs(extract_path, exist_ok=True)

# Unzip the file
with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print(f"‚úÖ Unzipped to: {extract_path}")

import numpy as np
import pickle
import os
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Re-load the file
padded_captions = np.load('/content/unzipped/cap_prep/padded_captions1.npy', allow_pickle=True)
print(f"Total captions: {len(padded_captions)}")

import pickle
import numpy as np
import tensorflow.keras as tf_keras
import sys

# Patch the import paths for pickle to resolve
sys.modules['keras.src'] = tf_keras
sys.modules['keras.src.preprocessing'] = tf_keras.preprocessing
sys.modules['keras.src.preprocessing.text'] = tf_keras.preprocessing.text

import pickle

# Now load the tokenizer
with open('/content/unzipped_folder/cap_prep/tokenizer1.pkl', 'rb') as f:
    tokenizer =¬†pickle.load(f)

# Load padded caption sequences
padded_captions = np.load('/content/unzipped/cap_prep/padded_captions1.npy', allow_pickle=True)
padded_captions

import pandas as pd
from IPython.display import display

# Step 1: Load the cleaned captions CSV
csv_path = '/content/unzipped/cap_prep/cleanedcaptions1.csv'
df = pd.read_csv(csv_path)

# Step 2: Display basic information
print("‚úÖ Captions CSV loaded successfully.")
print(f"Total records: {len(df)}")
print("Columns:", df.columns.tolist())

# Step 3: Show sample rows in grid/table format
print("\nüìä Preview of the first 10 captions:")
display(df.head(10))  # Displays in table format in Colab

captions_list = df['caption'].astype(str).tolist()

tokenizer.fit_on_texts(captions_list)

print("‚úÖ Tokenizer loaded.")
print(f"Total vocabulary size (including special tokens): {len(tokenizer.word_index) + 1}")

# Step 3: Add <start> and <end> tokens to tokenizer if missing
start_token = tokenizer.word_index.get('<start>')
end_token = tokenizer.word_index.get('<end>')

if start_token is None:
    tokenizer.word_index['<start>'] = len(tokenizer.word_index) + 1
    start_token = tokenizer.word_index['<start>']

if end_token is None:
    tokenizer.word_index['<end>'] = len(tokenizer.word_index) + 1
    end_token = tokenizer.word_index['<end>']

vocab_size = len(tokenizer.word_index) + 1

print("\n‚úÖ Special Tokens:")
print(f"<start> token index: {start_token}")
print(f"<end> token index: {end_token}")
print(f"Total vocabulary size (including special tokens): {vocab_size}")

# üîç Preview: What a tokenized + decoded caption looks like
sample_index = 0
original_caption = padded_captions[sample_index]
processed_caption = [start_token] + list(original_caption) + [end_token]
print("\nüîç Tokenized caption with special tokens:")
print("Token IDs:", processed_caption)

# Build index-to-word mapping for decoding
index_to_word = {v: k for k, v in tokenizer.word_index.items()}
decoded_caption = [index_to_word.get(token, "<UNK>") for token in processed_caption]
print("Decoded caption:", " ".join(decoded_caption))

from tensorflow.keras.preprocessing.sequence import pad_sequences

input_sequences = []
output_words = []

LIMIT = 5000  # Try smaller value if RAM crashes again
for caption in padded_captions[:LIMIT]:
    caption = [start_token] + list(caption) + [end_token]
    for i in range(1, len(caption)):
        input_seq = caption[:i]
        output_word = caption[i]
        input_sequences.append(input_seq)
        output_words.append(output_word)

max_length = max(len(seq) for seq in input_sequences)
X = pad_sequences(input_sequences, maxlen=max_length, padding='post')
y = np.array(output_words)

print("Subset data prepared.")
print("X shape:", X.shape)
print("y shape:", y.shape)

np.save('/content/unzipped/cap_prep/train_input_sequences.npy', X)
np.save('/content/unzipped/cap_prep/train_output_words.npy', y)

print("‚úÖ Training sequences saved.")





