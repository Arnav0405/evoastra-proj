# -*- coding: utf-8 -*-
"""prepared_sequences.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hao9rf0EsOWaGpiuyXs7VzmNS8CcUG-h
"""

import numpy as np
import pickle
import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import sys


tokenizer = Tokenizer(oov_token="<unk>", filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', lower=True)

import pandas as pd

# Step 1: Load the cleaned captions CSV
csv_path = 'cap_prep/cleanedcaptions1.csv'
df = pd.read_csv(csv_path)

# Step 2: Display basic information
print("âœ… Captions CSV loaded successfully.")
print(f"Total records: {len(df)}")
print("Columns:", df.columns.tolist())

# Step 3: Show sample rows in grid/table format
print("\nðŸ“Š Preview of the first 10 captions:")

captions_list = df['caption'].astype(str).tolist()

tokenizer.fit_on_texts(captions_list)

print("âœ… Tokenizer loaded.")
print(f"Total vocabulary size (including special tokens): {len(tokenizer.word_index) + 1}")

# Step 3: Add <start> and <end> tokens to tokenizer if missing
start_token = tokenizer.word_index.get('<start>')
end_token = tokenizer.word_index.get('<end>')

if start_token is None:
    tokenizer.word_index['<start>'] = len(tokenizer.word_index) + 1
    start_token = tokenizer.word_index['<start>']

if end_token is None:
    tokenizer.word_index['<end>'] = len(tokenizer.word_index) + 1
    end_token = tokenizer.word_index['<end>']

vocab_size = len(tokenizer.word_index) + 1

print("\nâœ… Special Tokens:")
print(f"<start> token index: {start_token}")
print(f"<end> token index: {end_token}")
print(f"Total vocabulary size (including special tokens): {vocab_size}")

# Build index-to-word mapping for decoding
index_to_word = {v: k for k, v in tokenizer.word_index.items()}

# Step 4: Load the padded captions
print("\nðŸ“¦ Loading padded captions...")
with open('cap_prep/padded_captions1.npy', 'rb') as f:
    padded_captions = np.load(f)

print(f"âœ… Padded captions loaded. Shape: {padded_captions.shape}")
print(f"Max caption length: {padded_captions.shape[1]}")

# Step 5: Create Input and Output sequences for LSTM training
print("\nðŸ”§ Creating input and output sequences for LSTM decoder...")

def create_sequences(captions, start_token, end_token):
    """
    Create input and output sequences for LSTM training.
    
    For each caption, we create multiple training examples:
    - Input sequence: [<start>, word1, word2, ..., wordn]
    - Output sequence: [word1, word2, ..., wordn, <end>]
    
    This teaches the model to predict the next word given the previous words.
    """
    input_sequences = []
    output_sequences = []
    
    for caption in captions:
        # Remove padding (0s) from the caption
        caption = [token for token in caption if token != 0]
        
        # Create the full sequence with start and end tokens
        full_sequence = [start_token] + caption + [end_token]
        
        # Create training examples for each position in the sequence
        for i in range(1, len(full_sequence)):
            # Input: from start up to current position
            input_seq = full_sequence[:i]
            # Output: the next word
            output_word = full_sequence[i]
            
            input_sequences.append(input_seq)
            output_sequences.append(output_word)
    
    return input_sequences, output_sequences

# Generate sequences
input_sequences, output_sequences = create_sequences(padded_captions, start_token, end_token)

print(f"âœ… Generated {len(input_sequences)} training examples")
print(f"Input sequences range: {min(len(seq) for seq in input_sequences)} to {max(len(seq) for seq in input_sequences)} tokens")

# Step 6: Pad input sequences to ensure uniform length
max_sequence_length = max(len(seq) for seq in input_sequences)
print(f"Max sequence length for padding: {max_sequence_length}")

# Pad input sequences
padded_input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')
print(f"âœ… Input sequences padded. Shape: {padded_input_sequences.shape}")

# Convert output sequences to numpy array
output_sequences = np.array(output_sequences)
print(f"âœ… Output sequences converted. Shape: {output_sequences.shape}")

# Step 7: Save the sequences for training
print("\nðŸ’¾ Saving sequences...")

# Save input sequences (decoder input)
# np.save('cap_prep/train_input_sequences.npy', padded_input_sequences)
print("âœ… Saved train_input_sequences.npy")

# Save output sequences (decoder target)
# np.save('cap_prep/train_output_words.npy', output_sequences)
print("âœ… Saved train_output_words.npy")

# Save important metadata
metadata = {
    'vocab_size': vocab_size,
    'max_sequence_length': max_sequence_length,
    'start_token': start_token,
    'end_token': end_token,
    'total_examples': len(input_sequences)
}

# with open('cap_prep/sequence_metadata.pkl', 'wb') as f:
    # pickle.dump(metadata, f)
print("âœ… Saved sequence_metadata.pkl")

# Step 8: Display examples
print("\nðŸ“Š Example training pairs:")
for i in range(3):
    input_seq = padded_input_sequences[i]
    output_word = output_sequences[i]
    
    # Decode input sequence
    input_words = [index_to_word.get(token, "<PAD>") for token in input_seq if token != 0]
    output_word_text = index_to_word.get(output_word, "<UNK>")
    
    print(f"\nExample {i+1}:")
    print(f"  Input:  {' '.join(input_words)}")
    print(f"  Target: {output_word_text}")
    print(f"  Input IDs: {input_seq[input_seq != 0].tolist()}")
    print(f"  Target ID: {output_word}")

with open('cap_prep/tokenizer1.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)
